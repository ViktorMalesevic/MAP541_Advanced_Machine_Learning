---
title: "R Notebook"
output: html_notebook
---

The objectives of the lab The purpose of this lab is to reproduce tables from the third chapter of the book "Elements of Statistical Learning" from Hastie, Tibshirani and Friedman, as they are shown bellow.

Ex. 1 — Tables 3.1 and 3.2 1. Prepare the data 

a) Rawdataisavailableonline,downloaditfrommoodle(theData.txtﬁle)orfromthe web at http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/ prostate.data. 

```{r}
data <- read.table("prostate.data.txt", sep = "") 
```

One value has to be corrected
```{r}
data[32,2] <- 3.8044
```


b) Extract and normalize the explicative variables

```{r}
X <- scale(data[,1:8])
```

c) Is it wise to normalize these data? 

I would say it depends on what we want to do. For a multilinear regression, it might not be needed. But for interpretation purposes it might be clever (eg PCA)

d) Extract the target variable 

```{r}
Y <- as.matrix(data[,"lpsa"])
```


e) Split the dataset into training and test data

```{r}
Xtrain <- X[data[["train"]], ] 
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ] 
```

2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1 

```{r}
Xtrainscale <- scale(Xtrain) 
C <- cov(as.matrix(Xtrainscale)) 
```

3. Reproduce the results presented Table 3.1

a) Compute the coeﬃcients of the linear regression model, without using the lm function (but you can use it validate your code)

```{r}
?solve
```


```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain) 
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```

b) Compute the prediction error

```{r}
Ypred <- Xtrainone %*% b 
err <- Ytrain - Ypred
```

c) Compute the standard error for each variable

```{r}
?diag
```


```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone)- ncol(X)-1) 
v <- diag(solve(t(Xtrainone) %*% Xtrainone)) 
stderr <- sqrt(as.vector(sig2)) * sqrt(v) 
sqrt(sum(stderr^2))
```

d) compute the Z score for each variable

```{r}
Z <- b/stderr
```

e) visualize the results and compare with table 3.2

```{r}
table32 <- cbind(b,stderr,Z)
table32
```

Exercise 2

1. Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and Lasso.

Let's take what we have done in the previous exercise:

```{r}
# The Least-Square (LS)
data <- read.table("prostate.data.txt", sep = "") 
data[32,2] <- 3.8044
X <- scale(data[,1:8])                   ### So far X is scaled but Y is not.
Y <- as.matrix(data[,"lpsa"])

Xtrain <- X[data[["train"]],]            ### The datasets extracted from X and Y are not scaled anymore.
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ]

Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)     ### Xtrain to which we add a column of '1' to find the intercept (using the theory of linear regression & mean square error)

b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
round(1000*b)/1000
```

Now let's try to find the same results using an appropriate package:

```{r}
#install.packages('CVXR')
```

```{r}
library(CVXR)
```

```{r}
# Least-Square with package CVXR

### Important remark: here we use Xtrainone again since we need the column of '1' for finding the intercept with this method.
p <- 9
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
LS <- round(1000*bo)/1000
LS
```

We oberve indeed that we find the same results as previously with our "manual" method. b and bo are identical. Here with the CVXR package, the method consists of posing a problem and giving a minimization objective (here minimizing the sum of least square errors :the usual objective when building a linear regression). However this approach helps building other minimization problems like the methods we have seen in class.
We find here the first column of Table 3.3 : LS (least square)

### Now let's use the CVRX method for retrieving the results of the Best Subset:

```{r}
# Best Subset

### Here we just take the 3 first regressors as suggested by the Table. We will try to retrieve this result in at the end of the exercise.
p <- 3                        
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone[,c(1,2,3)] %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
BS <-round(1000*bo)/1000
BS
BS <- rbind(BS, 0, 0, 0, 0, 0, 0)
```

We obtain the same results as Table 3.3

### Let's do the same for Ridge:

With the ridge one has to center X and take Beta0 = mean(Y).

Extract from the book the Elements of Statistical Learning pp63-64:

"The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving . In addition,notice that the intercept β0 has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for Y ; that is, adding a constant c to each of the targets yi would
not simply result in a shift of the predictions by the same amount c. It can be shown that the solution to can be separated
into two parts, after reparametrization using centered inputs: each xij gets
replaced by xij−x ̄j.We estimate β0 by mean(y).The remaining coefficients get estimated by a ridge regression without intercept, using the centered xij. Henceforth we assume that this centering has been done, so that the input matrix X has p (rather than p + 1) columns.

Writing the criterion in matrix form,
RSS(λ) = (y − Xβ)T (y − Xβ) + λβT β, 
the ridge regression solutions are easily seen to be
βˆridge = (XT X + λI)−1XT*y
"

Consequently we need to make sure that the new X is centered:

```{r}
Xtraincentered = Xtrain - mean(Xtrain)   ### Indeed since Xtrain is splitted after the scaling of X, it is not perfectly centered itself.
```

```{r}
# The Ridge
p <- 8
### Here the value of lambda is directly the one taken from the partial correction
lambda <- 24        

### Here in order to find the same results as the table, we need to scale 'Y' and 'X'.
Ytrainscale = scale(Ytrain)   
Xtrainscale = scale(Xtrain)

betaHat <- Variable(p)
objective <- Minimize(sum(((Ytrainscale-mean(Ytrainscale)) - Xtrainscale %*% betaHat)^2) + lambda*sum((betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

### However, be careful here: in order to find the coefficients we divide by the standard error of the unscaled data
d <- sqrt(diag(var(Xtrain)))    
br <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
round(1000*br)/1000
```

We observe here that our values don't correspond to the ones of the table. There must be a subtility we forget.
Let's try to do it "manually":

```{r}
# The Ridge (without CRVX package)
Xtraincentered = Xtrain - mean(Xtrain)   ### Just a repetition in case the chunks are not run in order.

### Here mean(Y) is taken for Beta 0.
br <- solve(t(Xtraincentered) %*% Xtraincentered + diag(x = 24, ncol(Xtraincentered)), t(Xtraincentered) %*% (Ytrain - mean(Ytrain)))                    
Ridge <- round(1000*br)/1000

### Here we add Beta 0 = mean(Y) to the set of coefficients.
Ridge <- rbind(round(1000*mean(Ytrain))/1000, Ridge)
Ridge
```

This time our result correspond indeed to the results of Table 3.3. (with 0.001 precision)

### Finally let's do this for the LASSO:

In the case of LASSO it is very often advised to scale the data.

```{r}
# The Lasso
p <- 8
### Here the value of t is directly the one taken from the partial correction
t <-  .7015    

### Here we need to scale both Ytrain and Xtrain
Ytrainscale = scale(Ytrain)   
Xtrainscale = scale(Xtrain)

betaHat <- Variable(p)
objective <- Minimize(sum((Ytrainscale - Xtrainscale %*% betaHat)^2))
constraint <- list(sum(abs(betaHat)) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

### However, be careful here: in order to find the coefficients we divide by the standard error of the unscaled data
d <- sqrt(diag(var(Xtrain)))   
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
Lasso <- round(1000*bl)/1000


### NOTA here: the Beta 0 we add is not mean(Y) this time but mean(Ypred-(Xtrain %*% bl)).
Lasso <- rbind(round(1000*mean(Ypred-(Xtrain %*% bl)))/1000, Lasso)
Lasso
```

Our results correspond to the one of Table 3.3.

### Let's now try to find the coefficients of the PCR:

For that we download and use the appropriate package called 'pls' which includes PCR and PLS models.

```{r}
#install.packages('pls')
```

```{r}
library(pls)
```

```{r}
# PCR
train <- as.data.frame(cbind(Ytrain, Xtrain))
pcr_model <- pcr(Ytrain ~ ., data = train)

PCR <- as.matrix(round(1000*pcr_model$coefficients[49:56])/1000)
PCR <- rbind(round(1000*mean(Ytrain))/1000, PCR)
PCR
```

Here we have similar results to Table 3.3 (with a difference of 0.006), so not exactly the same, but rather close.

### Let's now try to find the coefficients of the PLS:

```{r}
# PLS
train <- as.data.frame(cbind(Ytrain, Xtrain))
pls_model <- plsr(Ytrain ~ ., data = train)

PLS <- as.matrix(round(1000*pls_model$coefficients[9:16])/1000)
PLS <- rbind(round(1000*mean(Ytrain))/1000, PLS)
PLS
```

Our result correspond indeed to the results of table 3.3.

Let's concatenate the results together:

```{r}
# Results summarized in a table
table_results <- as.table(cbind(LS, BS, Ridge, Lasso, PCR, PLS))
rownames(table_results)[1] <- 'Intercept'
colnames(table_results) <- c('LS', '  Best Subset', '   Ridge', '   Lasso', '   PCR', '   PLS')
#table_results[table_results == 0] <- ''
table_results
```

All the columns are identical to the ones from Table 3.3 (up to 0.001 precision) a part from PCR, and the intercept of LASSO. However for both Ridge and LASSO, the course tells to take Beta0 = mean(Y)... And this is what we have done here. So I do not understand why there would be a difference in the intercept for LASSO (as well as for PCR by the way).



### Now let's calculate the error for each of the models:

REMINDER: In Xtest and Xtrain the colmuns of '1' are not included: thus the intercept has to be added manually after the multiplication of Beta*X.

```{r}
# LS Test error
Yhattest <-  Xtest %*% LS[-1] + LS[1]
Error <- Ytest - Yhattest
testError <- sum((Error)^2)/dim(Xtest)[1]
testError
```
This works.

Let's now see about the std error:

```{r}
# LS Std error
Yhattest <- Xtest %*% LS[-1] + LS[1]
Error <- Ytest - Yhattest
stdError <- sd((Error)^2)/sqrt(dim(Xtest)[1])
stdError
```
This works too.

Let's now create a function that will calculate these errors for each of our models:

```{r}
# The error calulation function
error <- function(Model){

  Yhattest <- Xtest %*% Model[-1] + Model[1]
  Error <- Ytest - Yhattest
  # Testing error
  testError <- sum((Error)^2)/dim(Xtest)[1]
  # Standard error
  stdError <- sd((Error)^2)/sqrt(dim(Xtest)[1])
  
  rbind(testError, stdError)
}
```

```{r}
round(1000*error(LS))/1000
round(1000*error(BS))/1000
round(1000*error(Ridge))/1000
round(1000*error(Lasso))/1000
round(1000*error(PCR))/1000
round(1000*error(PLS))/1000
```

We retrieve the same results as the table. (up to a 0.001 precision, a part for Ridge where our coefficient were a bit less precise)

Let's put it all in the previous Table:

```{r}
LS <- rbind(LS,round(1000*error(LS))/1000)
BS <- rbind(BS,round(1000*error(BS))/1000)
Ridge <- rbind(Ridge,round(1000*error(Ridge))/1000)
Lasso <- rbind(Lasso,round(1000*error(Lasso))/1000)
PCR <- rbind(PCR,round(1000*error(PCR))/1000)
PLS <- rbind(PLS,round(1000*error(PLS))/1000)
```

```{r}
# Results summarized in a Table
table_results <- as.table(cbind(LS, BS, Ridge, Lasso, PCR, PLS))
rownames(table_results)[1] <- 'Intercept'
rownames(table_results)[2:9] <- rownames(Ridge)[2:9]
rownames(table_results)[10] <- 'TestError'
rownames(table_results)[11] <- 'StdError'
colnames(table_results) <- c('LS', 'Best Subset', 'Ridge', 'Lasso', 'PCR', 'PLS')
table_results[table_results == 0] <- ''
table_results
```
 
We retrieve the same results as Table 3.3.





NOT USEFUL.
```{r}
# The Ridge

for (t in 0:10) {
  t <-  t/10
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

We see that anything above t = 0.7 does not change anymore.
Our value should be between 0.2 and 0.3

```{r}
for (t in 20:30) {
  t <-  t/100
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

The result is thus between 0.25 and 0.26

```{r}
for (t in 250:260) {
  t <-  t/1000
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

So it should be between .253 and .255

```{r}
t <-  0.254
ys = scale(Ytrain)
betaHat <- Variable(p-1)
objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
constraint <- list(sum((betaHat)^2) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

d <- sqrt(diag(var(Xtrain)))
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
print(round(1000*bl)/1000)
```

