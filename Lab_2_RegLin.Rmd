---
title: "R Notebook"
output: html_notebook
---

The objectives of the lab The purpose of this lab is to reproduce tables from the third chapter of the book "Elements of Statistical Learning" from Hastie, Tibshirani and Friedman, as they are shown bellow.

Ex. 1 — Tables 3.1 and 3.2 1. Prepare the data 

a) Rawdataisavailableonline,downloaditfrommoodle(theData.txtﬁle)orfromthe web at http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/ prostate.data. 

```{r}
data <- read.table("prostate.data.txt", sep = "") 
```

One value has to be corrected
```{r}
data[32,2] <- 3.8044
```


b) Extract and normalize the explicative variables

```{r}
X <- scale(data[,1:8])
```

c) Is it wise to normalize these data? 

I would say it depends on what we want to do. For a multilinear regression, it might not be needed. But for interpretation purposes it might be clever (eg PCA)

d) Extract the target variable 

```{r}
Y <- as.matrix(data[,"lpsa"])
```


e) Split the dataset into training and test data

```{r}
Xtrain <- X[data[["train"]], ] 
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ] 
```

2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1 

```{r}
Xtrainscale <- scale(Xtrain) 
C <- cov(as.matrix(Xtrainscale)) 
```

3. Reproduce the results presented Table 3.1

a) Compute the coeﬃcients of the linear regression model, without using the lm function (but you can use it validate your code)

```{r}
?solve
```


```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain) 
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```

b) Compute the prediction error

```{r}
Ypred <- Xtrainone %*% b 
err <- Ytrain - Ypred
```

c) Compute the standard error for each variable

```{r}
?diag
```


```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone)- ncol(X)-1) 
v <- diag(solve(t(Xtrainone) %*% Xtrainone)) 
stderr <- sqrt(as.vector(sig2)) * sqrt(v) 
sqrt(sum(stderr^2))
```

d) compute the Z score for each variable

```{r}
Z <- b/stderr
```

e) visualize the results and compare with table 3.2

```{r}
table32 <- cbind(b,stderr,Z)
table32
```

Exercise 2

1. Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and Lasso.

Let's take what we have done in the previous exercise:

```{r}
# The Least-Square (LS)
data <- read.table("prostate.data.txt", sep = "") 
data[32,2] <- 3.8044
X <- scale(data[,1:8])                   ### So far X is scaled but Y is not.
Y <- as.matrix(data[,"lpsa"])

Xtrain <- X[data[["train"]],]            ### The datasets extracted from X and Y are not scaled anymore.
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ]

Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)     ### Xtrain to which we add a column of '1' to find the intercept (using the theory of linear regression & mean square error)

b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
round(1000*b)/1000
```

Now let's try to find the same results using an appropriate package:

```{r}
#install.packages('CVXR')
```

```{r}
library(CVXR)
```

```{r}
# Least-Square with package CVXR

### Important remark: here we use Xtrainone again since we need the column of '1' for finding the intercept with this method.
p <- 9
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
LS <- round(1000*bo)/1000
LS
```

We oberve indeed that we find the same results as previously with our "manual" method. b and bo are identical. Here with the CVXR package, the method consists of posing a problem and giving a minimization objective (here minimizing the sum of least square errors :the usual objective when building a linear regression). However this approach helps building other minimization problems like the methods we have seen in class.
We find here the first column of Table 3.3 : LS (least square)

### Now let's use the CVRX method for retrieving the results of the Best Subset:

```{r}
# Best Subset

### Here we just take the 3 first regressors as suggested by the Table. We will try to retrieve this result in at the end of the exercise.
p <- 3                        
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone[,c(1,2,3)] %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
BS <-round(1000*bo)/1000
BS
BS <- rbind(BS, 0, 0, 0, 0, 0, 0)
```

We obtain the same results as Table 3.3

### Let's do the same for Ridge:

With the ridge one has to center X and take Beta0 = mean(Y).

Extract from the book the Elements of Statistical Learning pp63-64:

"The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving . In addition,notice that the intercept β0 has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for Y ; that is, adding a constant c to each of the targets yi would
not simply result in a shift of the predictions by the same amount c. It can be shown that the solution to can be separated
into two parts, after reparametrization using centered inputs: each xij gets
replaced by xij−x ̄j.We estimate β0 by mean(y).The remaining coefficients get estimated by a ridge regression without intercept, using the centered xij. Henceforth we assume that this centering has been done, so that the input matrix X has p (rather than p + 1) columns.

Writing the criterion in matrix form,
RSS(λ) = (y − Xβ)T (y − Xβ) + λβT β, 
the ridge regression solutions are easily seen to be
βˆridge = (XT X + λI)−1XT*y
"

Consequently we need to make sure that the new X is centered:

```{r}
Xtraincentered = Xtrain - mean(Xtrain)   ### Indeed since Xtrain is splitted after the scaling of X, it is not perfectly centered itself.
```

```{r}
# The Ridge
p <- 8
### Here the value of lambda is directly the one taken from the partial correction
lambda <- 24        

### Here in order to find the same results as the table, we need to scale 'Y' and 'X'.
Ytrainscale = scale(Ytrain)   
Xtrainscale = scale(Xtrain)

betaHat <- Variable(p)
objective <- Minimize(sum(((Ytrainscale-mean(Ytrainscale)) - Xtrainscale %*% betaHat)^2) + lambda*sum((betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

### However, be careful here: in order to find the coefficients we divide by the standard error of the unscaled data
d <- sqrt(diag(var(Xtrain)))    
br <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
round(1000*br)/1000
```

We observe here that our values don't correspond to the ones of the table. There must be a subtility we forget.
Let's try to do it "manually":

```{r}
# The Ridge (without CRVX package)
Xtraincentered = Xtrain - mean(Xtrain)   ### Just a repetition in case the chunks are not run in order.

### Here mean(Y) is taken for Beta 0.
br <- solve(t(Xtraincentered) %*% Xtraincentered + diag(x = 24, ncol(Xtraincentered)), t(Xtraincentered) %*% (Ytrain - mean(Ytrain)))                    
Ridge <- round(1000*br)/1000

### Here we add Beta 0 = mean(Y) to the set of coefficients.
Ridge <- rbind(round(1000*mean(Ytrain))/1000, Ridge)
Ridge
```

This time our result correspond indeed to the results of Table 3.3. (with 0.001 precision)

### Finally let's do this for the LASSO:

In the case of LASSO it is very often advised to scale the data.

```{r}
# The Lasso
p <- 8
### Here the value of t is directly the one taken from the partial correction
t <-  .7015    

### Here we need to scale both Ytrain and Xtrain
Ytrainscale = scale(Ytrain)   
Xtrainscale = scale(Xtrain)

betaHat <- Variable(p)
objective <- Minimize(sum((Ytrainscale - Xtrainscale %*% betaHat)^2))
constraint <- list(sum(abs(betaHat)) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

### However, be careful here: in order to find the coefficients we divide by the standard error of the unscaled data
d <- sqrt(diag(var(Xtrain)))   
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
Lasso <- round(1000*bl)/1000


### NOTA here: the Beta 0 we add is not mean(Y) this time but mean(Ypred-(Xtrain %*% bl)).
Lasso <- rbind(round(1000*mean(Ypred-(Xtrain %*% bl)))/1000, Lasso)
Lasso
```

Our results correspond to the one of Table 3.3.

### Let's now try to find the coefficients of the PCR:

For that we download and use the appropriate package called 'pls' which includes PCR and PLS models.

```{r}
#install.packages('pls')
```

```{r}
library(pls)
```

```{r}
# PCR
train <- as.data.frame(cbind(Ytrain, Xtrain))
pcr_model <- pcr(Ytrain ~ ., data = train)

PCR <- as.matrix(round(1000*pcr_model$coefficients[49:56])/1000)

### NOTA here: the Beta 0 we add is not mean(Y) this time but mean(Ypred-(Xtrain %*% PCR)).
PCR <- rbind(round(1000*mean(Ypred-(Xtrain %*% PCR)))/1000, PCR)
PCR
```

Here we have similar results to Table 3.3 (with a difference of 0.006), so not exactly the same, but rather close.

### Let's now try to find the coefficients of the PLS:

```{r}
# PLS
train <- as.data.frame(cbind(Ytrain, Xtrain))
pls_model <- plsr(Ytrain ~ ., data = train)

PLS <- as.matrix(round(1000*pls_model$coefficients[9:16])/1000)
PLS <- rbind(round(1000*mean(Ytrain))/1000, PLS)
PLS
```

Our result correspond indeed to the results of table 3.3.

Let's concatenate the results together:

```{r}
# Results summarized in a table
table_results <- as.table(cbind(LS, BS, Ridge, Lasso, PCR, PLS))
rownames(table_results)[1] <- 'Intercept'
colnames(table_results) <- c('LS', '  Best Subset', '   Ridge', '   Lasso', '   PCR', '   PLS')
#table_results[table_results == 0] <- ''
table_results
```

All the columns are identical to the ones from Table 3.3 (up to 0.001 precision) a part from PCR, and the intercept of LASSO. However for both Ridge and LASSO, the course tells to take Beta0 = mean(Y)... And this is what we have done here. So I do not understand why there would be a difference in the intercept for LASSO (as well as for PCR by the way).



### Now let's calculate the error for each of the models:

REMINDER: In Xtest and Xtrain the colmuns of '1' are not included: thus the intercept has to be added manually after the multiplication of Beta*X.

```{r}
# LS Test error
Yhattest <-  Xtest %*% LS[-1] + LS[1]
Error <- Ytest - Yhattest
testError <- sum((Error)^2)/dim(Xtest)[1]
testError
```
This works.

Let's now see about the std error:

```{r}
# LS Std error
Yhattest <- Xtest %*% LS[-1] + LS[1]
Error <- Ytest - Yhattest
stdError <- sd((Error)^2)/sqrt(dim(Xtest)[1])
stdError
```
This works too.

Let's now create a function that will calculate these errors for each of our models:

```{r}
# The error calulation function
error <- function(Model){

  Yhattest <- Xtest %*% Model[-1] + Model[1]
  Error <- Ytest - Yhattest
  # Testing error
  testError <- sum((Error)^2)/dim(Xtest)[1]
  # Standard error
  stdError <- sd((Error)^2)/sqrt(dim(Xtest)[1])
  
  rbind(testError, stdError)
}
```

```{r}
round(1000*error(LS))/1000
round(1000*error(BS))/1000
round(1000*error(Ridge))/1000
round(1000*error(Lasso))/1000
round(1000*error(PCR))/1000
round(1000*error(PLS))/1000
```

We retrieve the same results as the table. (up to a 0.001 precision, a part for Ridge where our coefficient were a bit less precise)

Let's put it all in the previous Table:

```{r}
LS <- rbind(LS,round(1000*error(LS))/1000)
BS <- rbind(BS,round(1000*error(BS))/1000)
Ridge <- rbind(Ridge,round(1000*error(Ridge))/1000)
Lasso <- rbind(Lasso,round(1000*error(Lasso))/1000)
PCR <- rbind(PCR,round(1000*error(PCR))/1000)
PLS <- rbind(PLS,round(1000*error(PLS))/1000)
```

```{r}
# Results summarized in a Table
table_results <- as.table(cbind(LS, BS, Ridge, Lasso, PCR, PLS))
rownames(table_results)[1] <- 'Intercept'
rownames(table_results)[2:9] <- rownames(Ridge)[2:9]
rownames(table_results)[10] <- 'TestError'
rownames(table_results)[11] <- 'StdError'
colnames(table_results) <- c('LS', 'Best Subset', 'Ridge', 'Lasso', 'PCR', 'PLS')
#table_results[table_results == 0] <- ''
table_results
```
 
We retrieve the same results as Table 3.3. (With a 0.001 precision, and a bit less precise for PCR)



# BONUS 1: Justify the number of parameters for the Best Subset

Let's see if we retrieve the Best-Subset with Backward-Stepwise selection:

# Best Subset with Backward-Stepwise selection

```{r}
#install.packages("faraway")
```

```{r}
library(faraway)
```

```{r}
df <- as.data.frame(data[data[['train']],1:9])
#df <- as.data.frame(data[1:9])
```

Backaward elimination example:

We eliminate the regressors with the biggest p-value one by one until we only have regressors with p-values < 0.05 (except the intercept)

```{r}
lmod <- lm(lpsa ~ ., data = df)
summary(lmod)
```

Here we eliminate gleason:

```{r}
lmod <- update(lmod,.~. - gleason)
summary(lmod)
```

Here we eliminate age:

```{r}
lmod <- update(lmod,.~. - age)
summary(lmod)
```

Here we eliminate lcp:

```{r}
lmod <- update(lmod,.~. - lcp)
summary(lmod)
```

Here we eliminate pgg45:

```{r}
lmod <- update(lmod,.~. - pgg45)
summary(lmod)
```

Here we eliminate lbph:

```{r}
lmod <- update(lmod,.~. - lbph)
summary(lmod)
```

Here the p-value for svi is still higher than 0.05 so we eliminate it:

```{r}
lmod <- update(lmod,.~. - svi)
summary(lmod)
```

Finally we obtain a model where only lcavol and lweight are used and are significant. And this actually corresponds to the results of Table 3.3.

# NOTA here: if we use the whole dataset and not only the training dataset results are slightly different: we have lcavol, lweight but also svi.

However, the Backwise-Stepwise method does not guaranty to find the actual Best-Subset. We might be stuck in a 'local minimum'

Let's then try a more 'global' method, where we ensure we do an exhaustive search of the Best-Subset:

# Best Subset exhaustive search using the package 'leaps' and information criterions (AIC & BIC)

```{r}
#install.packages('leaps')
```

```{r}
library(leaps)
```

Now let's define which is the best number of variables: in general AIC or BIC are examples of method being useful in that case.
However most of the time AIC is not 'strict' enough. This is why we will choose BIC:

```{r}
# BIC
df <- as.data.frame(data[data[['train']],1:9])
#df <- as.data.frame(data[1:9])

best.subset <- regsubsets(lpsa~., df)
best.subset.summary <- summary(best.subset)
best.subset.by.bic <- which.min(best.subset.summary$bic)


plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
```

The BIC tells us indeed that the best number of variables is 2.

Now let's display the best variables to be taken per number of variables chosen to build a model:

```{r}
best.subset.summary$outmat
```

Here we see indeed that if we choose two variables (decision taken thanks to BIC) lcavol and lweight are the best subset variables as displayed in Table 3.3.

# NOTA here: if we use the whole dataset and not only the training dataset results are slightly different: we retrieve the results of the Backward-Stepwise method and we have lcavol, lweight but also svi.

# BONUS 2: Find the optimal lambda and/or t for Ridge and Lasso:

NOT USEFUL.
```{r}
# The Ridge

for (t in 0:10) {
  t <-  t/10
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

We see that anything above t = 0.7 does not change anymore.
Our value should be between 0.2 and 0.3

```{r}
for (t in 20:30) {
  t <-  t/100
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

The result is thus between 0.25 and 0.26

```{r}
for (t in 250:260) {
  t <-  t/1000
  ys = scale(Ytrain)
  betaHat <- Variable(p-1)
  objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
  constraint <- list(sum((betaHat)^2) <=  t)
  problem <- Problem(objective, constraint)
  result <- solve(problem)

  d <- sqrt(diag(var(Xtrain)))
  bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
  print(round(1000*bl)/1000)
}
```

So it should be between .253 and .255

```{r}
t <-  0.254
ys = scale(Ytrain)
betaHat <- Variable(p-1)
objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
constraint <- list(sum((betaHat)^2) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

d <- sqrt(diag(var(Xtrain)))
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
print(round(1000*bl)/1000)
```

